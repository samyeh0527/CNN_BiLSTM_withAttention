{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_with_CnnBiGRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgbXpX2SWZM1/NU4XK9K/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyeh0527/CNN_BiLSTM_withAttention/blob/master/Attention_with_CnnBiGRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItNkCUnzESra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07e8cf6-7341-4b07-f8ff-965ea9d0fcec"
      },
      "source": [
        "!git clone https://github.com/samyeh0527/CNN_BiLSTM_withAttention.git\n",
        "%cd /content/CNN_BiLSTM_withAttention\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CNN_BiLSTM_withAttention'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 34 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n",
            "/content/CNN_BiLSTM_withAttention\n",
            "1210雙向注意力機制之深度學習模型應用於石油期貨價格之預測.pdf  Gold_lag4day.csv\n",
            "Gold_lag13day.csv\t\t\t\t\t      Gold_lag6day.csv\n",
            "Gold_lag16day.csv\t\t\t\t\t      Gold_lag8day.csv\n",
            "Gold_lag17day.csv\t\t\t\t\t      OilwithGold.csv\n",
            "Gold_lag1day.csv\t\t\t\t\t      README.md\n",
            "Gold_lag2day.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkd_2EneEZFx"
      },
      "source": [
        "#多维NormalizeMult返回數據和最大最小值\n",
        "import numpy \n",
        "import numpy as np\n",
        "\n",
        "numpy.random.seed(7)\n",
        "def NormalizeMult(data):\n",
        "    #normalize \n",
        "    data = np.array(data)\n",
        "    normalize = np.arange(2*data.shape[1],dtype='float64')\n",
        "\n",
        "    normalize = normalize.reshape(data.shape[1],2)\n",
        "    # print(normalize.shape)\n",
        "    for i in range(0,data.shape[1]):\n",
        "        list = data[:,i]\n",
        "        listlow,listhigh =  np.percentile(list, [0, 100])\n",
        "        # print(i)\n",
        "        normalize[i,0] = listlow\n",
        "        normalize[i,1] = listhigh\n",
        "        delta = listhigh - listlow\n",
        "        if delta != 0:\n",
        "            for j in range(0,data.shape[0]):\n",
        "                data[j,i]  =  (data[j,i] - listlow)/delta\n",
        "    #np.save(\"./normalize.npy\",normalize)\n",
        "    return  data,normalize\n",
        "\n",
        "#反正規\n",
        "def FNormalizeMult(data,normalize):\n",
        "    data = np.array(data)\n",
        "    for i in  range(0,data.shape[1]):\n",
        "        listlow =  normalize[i,0]\n",
        "        listhigh = normalize[i,1]\n",
        "        delta = listhigh - listlow\n",
        "        if delta != 0:\n",
        "            #第j行\n",
        "            for j in range(0,data.shape[0]):\n",
        "                data[j,i]  =  data[j,i]*delta + listlow\n",
        "\n",
        "    return data\n",
        "def attention_function(inputs, single_attention_vector=False):\n",
        "   #inputs.shape = (batch_size, TimeSteps, Dims) \n",
        "    \n",
        "    TimeSteps = K.int_shape(inputs)[1]\n",
        "    input_dim = K.int_shape(inputs)[2]\n",
        "    # print(\"TimeSteps\",TimeSteps)\n",
        "    # print(\"input_dim\",input_dim)\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Dense(TimeSteps, activation='softmax')(a)\n",
        "    if single_attention_vector:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "\n",
        "    a_probs = Permute((2, 1))(a)\n",
        "    # element * wise\n",
        "    output_attention_mul = Multiply()([inputs, a_probs])\n",
        "    return output_attention_mul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbfNXmxLEPhP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "3f60b869-0990-48d2-a40c-1d2ddc0a1251"
      },
      "source": [
        "from keras.layers import Input, Dense, LSTM, merge ,Conv1D,Dropout,Bidirectional,Multiply,Concatenate,BatchNormalization,GRU\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.layers import merge\n",
        "from keras.layers.core import *\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import *\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers\n",
        "import numpy\n",
        "import  pandas as pd\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM,Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras import backend as K\n",
        "\n",
        "starttime = datetime.datetime.now()\n",
        "def NormalizeMult(data):\n",
        "    #normalize \n",
        "    data = np.array(data)\n",
        "    normalize = np.arange(2*data.shape[1],dtype='float64')\n",
        "\n",
        "    normalize = normalize.reshape(data.shape[1],2)\n",
        "    # print(normalize.shape)\n",
        "    for i in range(0,data.shape[1]):\n",
        "        list = data[:,i]\n",
        "        listlow,listhigh =  np.percentile(list, [0, 100])\n",
        "        # print(i)\n",
        "        normalize[i,0] = listlow\n",
        "        normalize[i,1] = listhigh\n",
        "        delta = listhigh - listlow\n",
        "        if delta != 0:\n",
        "            for j in range(0,data.shape[0]):\n",
        "                data[j,i]  =  (data[j,i] - listlow)/delta\n",
        "    #np.save(\"./normalize.npy\",normalize)\n",
        "    return  data,normalize\n",
        "\n",
        "#反正規\n",
        "def FNormalizeMult(data,normalize):\n",
        "    data = np.array(data)\n",
        "    for i in  range(0,data.shape[1]):\n",
        "        listlow =  normalize[i,0]\n",
        "        listhigh = normalize[i,1]\n",
        "        delta = listhigh - listlow\n",
        "        if delta != 0:\n",
        "            #第j行\n",
        "            for j in range(0,data.shape[0]):\n",
        "                data[j,i]  =  data[j,i]*delta + listlow\n",
        "\n",
        "    return data\n",
        "def attention_function(inputs, single_attention_vector=False):\n",
        "   #inputs.shape = (batch_size, TimeSteps, Dims) \n",
        "    \n",
        "    TimeSteps = K.int_shape(inputs)[1]\n",
        "    input_dim = K.int_shape(inputs)[2]\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Dense(TimeSteps, activation='softmax')(a)\n",
        "    if single_attention_vector:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "\n",
        "    a_probs = Permute((2, 1))(a)\n",
        "    # element * wise\n",
        "    output_attention_mul = Multiply()([inputs, a_probs])\n",
        "    return output_attention_mul\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset2(dataset, look_back):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back),:]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back,:])\n",
        "    TrainX = numpy.array(dataX)\n",
        "    Train_Y = numpy.array(dataY)\n",
        "\n",
        "    return TrainX, Train_Y \n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset\n",
        "dataframe = read_csv('/content/CNN_BiLSTM_withAttention/OilwithGold.csv')\n",
        "dataframe = dataframe.drop(['Date'], axis = 1)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "testVaild=dataset\n",
        "trainVaild=dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.7)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "testVaild = testVaild[train_size:len(dataset),:]\n",
        "trainVaild = trainVaild[:train_size,:]\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "# normalize the dataset\n",
        "dataset,da_normalize = NormalizeMult(dataset)\n",
        "train,tr_normalize = NormalizeMult(train)\n",
        "test,te_normalize = NormalizeMult(test)\n",
        "\n",
        "\n",
        "look_back = 2\n",
        "TimeSteps=look_back\n",
        "Dims=2\n",
        "trainX, trY = create_dataset2(train, look_back)\n",
        "testX, teY = create_dataset2(test, look_back)\n",
        "print(trainX.shape,trY.shape)\n",
        "print(testX.shape,teY.shape)\n",
        "trainY =trY[:,0]\n",
        "testY =teY[:,0]\n",
        "print(trainY.shape)\n",
        "print(testY.shape)\n",
        "\n",
        "\n",
        "def attention_model():\n",
        "    inputs = Input(shape=(TimeSteps, Dims))\n",
        "    x = Conv1D(filters = 128, kernel_size = 1, activation = 'relu')(inputs)  \n",
        "    BiGRU_out = Bidirectional(GRU(64, return_sequences=True,activation=\"relu\"))(x)\n",
        "    Batch_Normalization = BatchNormalization()(BiGRU_out)\n",
        "    Drop_out = Dropout(0.1)(Batch_Normalization)\n",
        "    attention = attention_function(Drop_out)\n",
        "    Batch_Normalization = BatchNormalization()(attention)\n",
        "    Drop_out = Dropout(0.1)(Batch_Normalization)\n",
        "    Flatten_ = Flatten()(Drop_out)\n",
        "    output=Dropout(0.1)(Flatten_)\n",
        "    output = Dense(1, activation='sigmoid')(output)\n",
        "    model = Model(inputs=[inputs], outputs=output)\n",
        "    return model\n",
        "\n",
        "m = attention_model()\n",
        "m.summary()\n",
        "m.compile(loss='mean_squared_error', optimizer='adam')\n",
        "m.fit(trainX, trainY, epochs=30, batch_size=64, verbose=0,validation_data=(testX, testY))\n",
        "# make predictions\n",
        "trainPredict = m.predict(trainX)\n",
        "testPredict = m.predict(testX)\n",
        "#FNormalize\n",
        "trainPredict_FNormalizeMult= FNormalizeMult(trainPredict,tr_normalize)\n",
        "testPredict_FNormalizeMult= FNormalizeMult(testPredict,te_normalize)\n",
        "\n",
        "#calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\n",
        "\n",
        "print('*  Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY, testPredict))\n",
        "print('*  Test Score: %.2f RMSE' % (testScore))\n",
        "teV=testVaild[:-(look_back+1),0] \n",
        "trV=trainVaild[:-(look_back+1),0] \n",
        "print(\"---\")\n",
        "print(\"*  look_back = \",look_back)\n",
        "trainScore = math.sqrt(mean_squared_error(trV, trainPredict_FNormalizeMult))\n",
        "print('*  FNormalizeMult Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(teV, testPredict_FNormalizeMult))\n",
        "print('*  FNormalizeMult Test Score: %.2f RMSE' % (testScore))\n",
        "# shift test predictions for plotting\n",
        "                                       \n",
        "plt.plot(teV)       \n",
        "plt.plot(testPredict_FNormalizeMult,'r')\n",
        "plt.show()\n",
        "plot_model(m, show_shapes=True, to_file='AttentionCnnBiGRU.png')\n",
        "plt.plot(trV)\n",
        "plt.plot(trainPredict_FNormalizeMult,'r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1adad4d29a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_model' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0GN9UNZOpA7"
      },
      "source": [
        "import csv\n",
        "\n",
        "# 開啟輸出的 CSV 檔案\n",
        "with open('testPredict_FNormalizeMult.csv', 'w', newline='') as csvfile:\n",
        "  # 建立 CSV 檔寫入器\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerows(testPredict_FNormalizeMult)\n",
        "\n",
        "with open('trainPredict_FNormalizeMult.csv', 'w', newline='') as csvfile:\n",
        "  # 建立 CSV 檔寫入器\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerows(trainPredict_FNormalizeMult)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8XgOU2gI-7w"
      },
      "source": [
        "96 train 4 test Att with CNNBiGRU\n",
        "---\n",
        "*  look_back =  1\n",
        "*  Train Score: 0.04 RMSE\n",
        "*  Test Score: 0.08 RMSE\n",
        "*  FNormalizeMult Test Score: 3.60 RMSE\n",
        "---\n",
        "* loob_back = 2\n",
        "* Train Score: 0.02 RMSE\n",
        "* Test Score: 0.07 RMSE\n",
        "* FNormalizeMult Test Score: 3.79 RMSE\n",
        "---\n",
        "*  look_back =  3\n",
        "*  Train Score: 0.01 RMSE\n",
        "*  Test Score: 0.07 RMSE\n",
        "*  FNormalizeMult Test Score: 3.23 RMSE\n",
        "---\n",
        "*  look_back =  4\n",
        "*  Train Score: 0.03 RMSE\n",
        "*  Test Score: 0.09 RMSE\n",
        "*  FNormalizeMult Test Score: 4.77 RMSE\n",
        "---\n",
        "*  look_back =  5\n",
        "*  Train Score: 0.03 RMSE\n",
        "*  Test Score: 0.10 RMSE\n",
        "*  FNormalizeMult Test Score: 5.24 RMSE\n",
        "---\n",
        "*  look_back =  6\n",
        "*  Train Score: 0.02 RMSE\n",
        "*  Test Score: 0.09 RMSE\n",
        "*  FNormalizeMult Test Score: 6.33 RMSE\n",
        "---\n",
        "*  look_back =  7\n",
        "*  Train Score: 0.02 RMSE\n",
        "*  Test Score: 0.06 RMSE\n",
        "*  FNormalizeMult Test Score: 5.77 RMSE\n",
        "---\n",
        "*  look_back =  8\n",
        "*  Train Score: 0.02 RMSE\n",
        "*  Test Score: 0.10 RMSE\n",
        "*  FNormalizeMult Test Score: 6.88 RMSE\n",
        "---\n",
        "*  look_back =  9\n",
        "*  Train Score: 0.03 RMSE\n",
        "*  Test Score: 0.08 RMSE\n",
        "*  FNormalizeMult Test Score: 7.42 RMSE\n",
        "---\n",
        "*  look_back =  10\n",
        "*  Train Score: 0.03 RMSE\n",
        "*  Test Score: 0.08 RMSE\n",
        "*  FNormalizeMult Test Score: 7.85 RMSE\n",
        "---\n",
        "\n",
        "*  look_back =  11\n",
        "*  FNormalizeMult Train Score: 5.05 RMSE\n",
        "*  FNormalizeMult Test Score: 8.88 RMSE\n",
        "---\n",
        "*  look_back =  12\n",
        "*  FNormalizeMult Train Score: 4.38 RMSE\n",
        "*  FNormalizeMult Test Score: 9.44 RMSE\n",
        "---\n",
        "*  look_back =  13\n",
        "\n",
        "*  FNormalizeMult Test Score: 10.07RMSE\n",
        "---\n",
        "*  look_back =  14\n",
        "*  FNormalizeMult Train Score: 4.59 RMSE\n",
        "*  FNormalizeMult Test Score: 10.77 RMSE\n",
        "---\n",
        "*  look_back =  15\n",
        "*  FNormalizeMult Train Score: 4.63 RMSE\n",
        "*  FNormalizeMult Test Score: 11.00 RMSE\n",
        "---\n",
        "---\n",
        "*  look_back =  16\n",
        "*  FNormalizeMult Train Score: 5.08 RMSE\n",
        "*  FNormalizeMult Test Score: 11.38 RMSE\n",
        "---\n",
        "*  look_back =  17\n",
        "*  FNormalizeMult Train Score: 7.89 RMSE\n",
        "*  FNormalizeMult Test Score: 11.93 RMSE\n",
        "---\n",
        "*  look_back =  18\n",
        "*  FNormalizeMult Train Score: 5.43 RMSE\n",
        "*  FNormalizeMult Test Score: 12.22 RMSE\n",
        "---\n",
        "*  look_back =  19\n",
        "*  FNormalizeMult Train Score: 6.17 RMSE\n",
        "*  FNormalizeMult Test Score: 12.32 RMSE\n",
        "---\n",
        "*  look_back =  19\n",
        "*  FNormalizeMult Train Score: 6.17 RMSE\n",
        "*  FNormalizeMult Test Score: 12.32 RMSE\n",
        "---\n",
        "*  look_back =  20\n",
        "*  FNormalizeMult Train Score: 6.24 RMSE\n",
        "*  FNormalizeMult Test Score: 12.95 RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krXZcSBEPaTp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ7bH7PFJ-hq"
      },
      "source": [
        ""
      ]
    }
  ]
}